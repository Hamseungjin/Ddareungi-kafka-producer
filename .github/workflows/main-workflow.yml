name: deploy to kafka broker
on:
  # 어떤 브랜치에서 어떤 동작 발생시 actions 를 실행시킬 것인지 정의
  # 실습에서는 main branch 에 push가 발생했을 때로 정의함
  push:
    branches: [ main ]

  workflow_dispatch:

# job -> step -> action 순서로 정의
jobs:
  # build 라는 job을 정의
  build:
    # github에서 제공하는 ubuntu 환경에서 실행
    # runs-on 항목 아래부터 step 정의
    runs-on: ubuntu-latest
    steps:
    - name: checkout release
      # uses: 이미 만들어진 action을 사용할 때 사용
      # actions/checkout@4는 본 레파지토리의 내용을 checkout 하여 runner 서버에 저장함
      uses: actions/checkout@v4

    # 테스트에 대한 설정 필요 승진님과 논의 필요
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m venv venv
        source venv/bin/activate
        pip install --upgrade pip
        pip install python-dotenv
        pip install -r requirements.txt

    - name: Run tests
      run: |
        source venv/bin/activate
        python tests/test.py

    - name: archive to tar.gz
      run: tar cvfz ./Ddareungi-kafka-producer.tar.gz *

    - name: AWS configure credentials
      # aws 접속시 필요한 키 셋팅하는 사전 정의된 action 사용
      uses: aws-actions/configure-aws-credentials@v1
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ap-northeast-2

    - name: upload to S3
      run: aws s3 cp --region ap-northeast-2 ./Ddareungi-kafka-producer.tar.gz s3://datalake-actions-deploy-kafka-producer/kafka-producer/Ddareungi-kafka-producer.tar.gz

    - name: deploy with AWS codeDeploy
    # 대한님과 해당 부분에 대한 논의 필요
      run: aws deploy create-deployment
        --application-name datalake-deploy 
        --deployment-group-name kafka-deploy
        --s3-location bucket=datalake-actions-deploy-kafka-producer,bundleType=tgz,key=kafka-producer/Ddareungi-kafka-producer.tar.gz
